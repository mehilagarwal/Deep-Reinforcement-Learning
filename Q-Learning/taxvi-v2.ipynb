{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+\n|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n| : |\u001b[43m \u001b[0m: : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\n"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "qtable = np.zeros((state_size,action_size))\n",
    "qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000 # num of train eps\n",
    "test_eps = 100 # num of test eps\n",
    "max_steps = 99 # after how many steps should the episode end\n",
    "show_every = 1000 #when to render environment\n",
    "show = True\n",
    "alpha = 0.7  # learning rate to adjust the update of q table values(action state values)\n",
    "gamma = 0.6  # discount rate of reward\n",
    "\n",
    "epsilon = 1  # epsilon value for epsilon greedy policy\n",
    "max_epsilon = 1 # max val of epsilon\n",
    "min_epsilon = 0.1 # min val of epsilon\n",
    "decay_rate = 0.1 # the rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in range(total_episodes):\n",
    "    state = env.reset()  # getting initial state of env\n",
    "    step = 0  # intializing num of steps\n",
    "    done = False  \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # genearte a exploit vs explore random prob value to decide on action to explore or exploit according to the epsilon value\n",
    "        exploit = random.uniform(0,1)   \n",
    "        # deciding on explore vs exploit\n",
    "        if exploit > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        #getting next state, reward, episode termination boolean and general info after taking action in state\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #rendering env at show_ebery num of episodes\n",
    "        # if show and not eps%show_every:\n",
    "        #     env.render()\n",
    "        #updating q value acodring to SARSA max algo(q-learning)\n",
    "        qtable[state, action] = qtable[state, action] + alpha*(reward + gamma*(np.max(qtable[new_state,:])) - qtable[state,action])\n",
    "        \n",
    "        #updating new state\n",
    "        state = new_state\n",
    "        #checking if episode terminated   \n",
    "        if done:\n",
    "            break\n",
    "    # updating epsilon at the end of an episode\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Reward in  0  episode is  4\nReward in  1  episode is  12\nReward in  2  episode is  8\nReward in  3  episode is  7\nReward in  4  episode is  7\nReward in  5  episode is  4\nReward in  6  episode is  9\nReward in  7  episode is  6\nReward in  8  episode is  7\nReward in  9  episode is  7\nReward in  10  episode is  7\nReward in  11  episode is  11\nReward in  12  episode is  12\nReward in  13  episode is  8\nReward in  14  episode is  3\nReward in  15  episode is  9\nReward in  16  episode is  6\nReward in  17  episode is  7\nReward in  18  episode is  13\nReward in  19  episode is  12\nReward in  20  episode is  7\nReward in  21  episode is  3\nReward in  22  episode is  7\nReward in  23  episode is  9\nReward in  24  episode is  13\nReward in  25  episode is  5\nReward in  26  episode is  9\nReward in  27  episode is  6\nReward in  28  episode is  3\nReward in  29  episode is  14\nReward in  30  episode is  8\nReward in  31  episode is  14\nReward in  32  episode is  9\nReward in  33  episode is  7\nReward in  34  episode is  8\nReward in  35  episode is  4\nReward in  36  episode is  5\nReward in  37  episode is  6\nReward in  38  episode is  12\nReward in  39  episode is  7\nReward in  40  episode is  12\nReward in  41  episode is  10\nReward in  42  episode is  7\nReward in  43  episode is  10\nReward in  44  episode is  7\nReward in  45  episode is  6\nReward in  46  episode is  11\nReward in  47  episode is  8\nReward in  48  episode is  7\nReward in  49  episode is  7\nReward in  50  episode is  10\nReward in  51  episode is  7\nReward in  52  episode is  10\nReward in  53  episode is  11\nReward in  54  episode is  7\nReward in  55  episode is  7\nReward in  56  episode is  5\nReward in  57  episode is  4\nReward in  58  episode is  7\nReward in  59  episode is  7\nReward in  60  episode is  6\nReward in  61  episode is  7\nReward in  62  episode is  9\nReward in  63  episode is  8\nReward in  64  episode is  8\nReward in  65  episode is  5\nReward in  66  episode is  9\nReward in  67  episode is  10\nReward in  68  episode is  14\nReward in  69  episode is  9\nReward in  70  episode is  9\nReward in  71  episode is  9\nReward in  72  episode is  12\nReward in  73  episode is  4\nReward in  74  episode is  9\nReward in  75  episode is  10\nReward in  76  episode is  7\nReward in  77  episode is  6\nReward in  78  episode is  11\nReward in  79  episode is  11\nReward in  80  episode is  9\nReward in  81  episode is  4\nReward in  82  episode is  10\nReward in  83  episode is  7\nReward in  84  episode is  7\nReward in  85  episode is  6\nReward in  86  episode is  7\nReward in  87  episode is  5\nReward in  88  episode is  10\nReward in  89  episode is  9\nReward in  90  episode is  8\nReward in  91  episode is  7\nReward in  92  episode is  8\nReward in  93  episode is  15\nReward in  94  episode is  11\nReward in  95  episode is  6\nReward in  96  episode is  10\nReward in  97  episode is  15\nReward in  98  episode is  7\nReward in  99  episode is  12\n\n\nScore over time: 8.21\nMaximum reward was  15\n"
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "for t_eps in range(test_eps):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        #env.render()\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            rewards.append(total_reward)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    print('Reward in ', t_eps ,' episode is ', total_reward)\n",
    "    \n",
    "env.close()\n",
    "print()\n",
    "print()\n",
    "print(\"Score over time: \" +  str(sum(rewards)/test_eps))\n",
    "print('Maximum reward was ', max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}